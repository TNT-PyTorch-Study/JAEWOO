{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH7_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bg1J6BrJWNAV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기본 Transformer 구조"
      ],
      "metadata": {
        "id": "P6JJSXJMdzJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p):\n",
        "    super.__init__()\n",
        "\n",
        "    self.transformer = nn.Transformer(\n",
        "        d_model = dim_model,\n",
        "        nhead = num_heads,\n",
        "        num_encoder_layers = num_encoder_layers,\n",
        "        num_decoder_layers = num_decoder_layers,\n",
        "        dropout = dropout_p\n",
        "    )\n",
        "\n",
        "  def forward(self):\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "M7EJCJ4NdfcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "1UTPIlCxi2mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, dim_model, dropout_p, max_len):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    \n",
        "    # Encoding - Form formula\n",
        "    pos_encoding = torch.zeros(max_len, dim_model)\n",
        "    positions_list = torch.arange(0, max_len, dtype = torch.float).view(-1,1) # position은 홀 짝 번갈아 나온다.\n",
        "    division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.))/dim_model) # div term은 짝수로 고정이다. \n",
        "\n",
        "    pos_encoding[:, 0::2] = torch.sin(positions_list * division_term) # 짝수의 경우에는 sin\n",
        "    pos_encoding[:, 1::2] = torch.cos(positions_list * division_term) # 짝수의 경우에는 cos\n",
        "\n",
        "    # Saving Buffer\n",
        "    pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1) # flatten후 행벡터로 변환\n",
        "    self.register_buffer(\"pos_encoding\", pos_encoding)\n",
        "\n",
        "    # register_buffer 로 layer를 등록하면 어떤 특징이 있는가?\n",
        "    # 1. optimizer가 업데이트하지 않는다.\n",
        "    # 2. 그러나 값은 존재한다(하나의 layer로써 작용한다고 보면 된다.)\n",
        "    # 3. state_dict()로 확인이 가능하다.\n",
        "    # 4. GPU연산이 가능하다.\n",
        "    #따라서 네트워크를 구성함에 있어서 네트워크를 end2end로 학습시키고 싶은데 중간에 업데이트를 하지않는 일반 layer를 넣고 싶을 때 사용할 수 있다.\n",
        "\n",
        "  def forward(self, token_embedding : torch.tensor) -> torch.tensor: # ->??\n",
        "    return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0),:] ) # nn.Dropout의 연산은 20%를 0으로 만들어 버린다. \n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "F2DcC-GUi6Cw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p):\n",
        "    super().__init__()\n",
        "    self.model_type = \"Transformer\"\n",
        "    self.dim_model = dim_model\n",
        "    self.positional_encoder = PositionalEncoding(dim_model = dim_model, dropout_p = dropout_p, max_len = 5000) # 학습 불가능 한 pos encoding 층\n",
        "    self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "    self.transformer = nn.Transformer(\n",
        "        d_model = dim_model,\n",
        "        nhead = num_heads,\n",
        "        num_encoder_layers = num_encoder_layers,\n",
        "        num_decoder_layers = num_decoder_layers,\n",
        "        dropout = dropout_p\n",
        "    )\n",
        "    self.out = nn.Linear(dim_model, num_tokens)\n",
        "\n",
        "  def forward(self, src, tgt, tgt_mask = None, src_pad_mask = None, tgt_pad_mask = None):\n",
        "    # src, tgt 의 size는 반드시 (batch_size, src squence length)이어야 한다.\n",
        "    # Embedding + pos encoding - out size = (batch_size, sequence length, dim_model)\n",
        "\n",
        "    src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "    tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "    src = self.positional_encoder(src)\n",
        "    tgt = self.positional_encoder(tgt)\n",
        "\n",
        "    src = src.permute(1,0,2)\n",
        "    tgt = tgt.permute(1,0,2)\n",
        "\n",
        "    transformer_out = self.transformer(src, tgt, tgt_mask = tgt_mask, src_key_padding_mask = src_pad_mask, tgt_key_padding_mask = tgt_pad_mask)\n",
        "    out = self.out(transformer_out)\n",
        "    return out\n",
        "\n",
        "  def get_tgt_mask(self, size):\n",
        "    tgt_mask = torch.tril(torch.ones(size,size) == 1)\n",
        "    tgt_mask = tgt_mask.float()\n",
        "    tgt_mask = tgt_mask.masked_fill(tgt_mask == 0 , float('-inf')) # 무한대를 정의하는 방법이구나. \n",
        "    tgt_mask = tgt_mask.masked_fill(tgt_mask == 0 , float(0))\n",
        "    return tgt_mask\n",
        "\n",
        "  def creat_pad_mask(self, matrix : torch.tensor, pad_token : int) -> torch.tensor :\n",
        "    return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "6p6L8qNMoCQC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_data(n):\n",
        "    SOS_token = np.array([2])\n",
        "    EOS_token = np.array([3])\n",
        "    length = 8\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
        "    for i in range(n // 3):\n",
        "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
        "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
        "        data.append([X, y])\n",
        "\n",
        "    # 0,0,0,0 -> 0,0,0,0\n",
        "    for i in range(n // 3):\n",
        "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
        "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
        "        data.append([X, y])\n",
        "\n",
        "    # 1,0,1,0 -> 1,0,1,0,1\n",
        "    for i in range(n // 3):\n",
        "        X = np.zeros(length)\n",
        "        start = random.randint(0, 1)\n",
        "\n",
        "        X[start::2] = 1\n",
        "\n",
        "        y = np.zeros(length)\n",
        "        if X[-1] == 0:\n",
        "            y[::2] = 1\n",
        "        else:\n",
        "            y[1::2] = 1\n",
        "\n",
        "        X = np.concatenate((SOS_token, X, EOS_token))\n",
        "        y = np.concatenate((SOS_token, y, EOS_token))\n",
        "        data.append([X, y])\n",
        "\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "SVc2m0vtru6_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # batch_size 크기가 아닌 경우 마지막 비트를 얻지 않도록 합니다.\n",
        "        if idx + batch_size < len(data):\n",
        "            # 여기서 배치의 최대 길이를 가져와 PAD 토큰으로 길이를 정규화해야 합니다.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "                # batch에서 가장 긴 문장 가져오기\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq) > max_batch_length:\n",
        "                        max_batch_length = len(seq)\n",
        "\n",
        "                # 최대 길이에 도달할 때까지 X 패딩 토큰을 추가합니다.\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
        "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
        "\n",
        "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
        "\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches\n",
        "\n",
        "\n",
        "train_data = generate_random_data(9000)\n",
        "val_data = generate_random_data(3000)\n",
        "\n",
        "train_dataloader = batchify_data(train_data)\n",
        "val_dataloader = batchify_data(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GXjbO-crxv3",
        "outputId": "47b7a3f0-b671-40fe-fe6f-6f4226ba5610"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "562 batches of size 16\n",
            "187 batches of size 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1).to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "AkL68pbRtEb1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습"
      ],
      "metadata": {
        "id": "oDTQqltCwkTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    X,y = batch[:, 0], batch[:, 1]\n",
        "    X,y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
        "\n",
        "    y_input = y[:,:-1] # SOS 부터 다 넣는다.\n",
        "    y_expected = y[:, 1:] # EOS를 빼고 예측한다.\n",
        "\n",
        "    sequence_length = y_input.size(1)\n",
        "    tgt_mask = model.get_tgt_mask(sequence_length).to(device) # 아까 클래스에 구현해 둔 mask 생성함수\n",
        "\n",
        "    pred = model(X, y_input, tgt_mask) \n",
        "    pred = pred.permute(1,2,0)\n",
        "    loss = loss_fn(pred, y_expected)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.detach().item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:]\n",
        "\n",
        "            sequence_length = y_input.size(1)\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            pred = pred.permute(1, 2, 0)      \n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "MRVz-5qPuGKn"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):  \n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return train_loss_list, validation_loss_list\\"
      ],
      "metadata": {
        "id": "4UMONCwkx23P"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsRBq_dUySLc",
        "outputId": "0ffda577-43de-4c28-9c72-001a57be2595"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n",
            "Training loss: 0.3896\n",
            "Validation loss: 0.3410\n",
            "\n",
            "------------------------- Epoch 2 -------------------------\n",
            "Training loss: 0.3582\n",
            "Validation loss: 0.2987\n",
            "\n",
            "------------------------- Epoch 3 -------------------------\n",
            "Training loss: 0.3227\n",
            "Validation loss: 0.2634\n",
            "\n",
            "------------------------- Epoch 4 -------------------------\n",
            "Training loss: 0.2982\n",
            "Validation loss: 0.2319\n",
            "\n",
            "------------------------- Epoch 5 -------------------------\n",
            "Training loss: 0.2784\n",
            "Validation loss: 0.2145\n",
            "\n",
            "------------------------- Epoch 6 -------------------------\n",
            "Training loss: 0.2676\n",
            "Validation loss: 0.2033\n",
            "\n",
            "------------------------- Epoch 7 -------------------------\n",
            "Training loss: 0.2563\n",
            "Validation loss: 0.1936\n",
            "\n",
            "------------------------- Epoch 8 -------------------------\n",
            "Training loss: 0.2490\n",
            "Validation loss: 0.1848\n",
            "\n",
            "------------------------- Epoch 9 -------------------------\n",
            "Training loss: 0.2417\n",
            "Validation loss: 0.1741\n",
            "\n",
            "------------------------- Epoch 10 -------------------------\n",
            "Training loss: 0.2370\n",
            "Validation loss: 0.1677\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, max_length=15, SOS_token=2, EOS_token=3):\n",
        "    model.eval()\n",
        "\n",
        "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
        "\n",
        "    num_tokens = len(input_sequence[0])\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get source mask\n",
        "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "\n",
        "        pred = model(input_sequence, y_input, tgt_mask)\n",
        "\n",
        "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
        "        next_item = torch.tensor([[next_item]], device=device)\n",
        "\n",
        "        # Concatenate previous input with predicted best word\n",
        "        y_input = torch.cat((y_input, next_item), dim=1)\n",
        "\n",
        "        # Stop if model predicts end of sentence\n",
        "        if next_item.view(-1).item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    return y_input.view(-1).tolist()\n",
        "\n",
        "\n",
        "# Here we test some examples to observe how the model predicts\n",
        "examples = [\n",
        "    torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]], dtype=torch.long, device=device),\n",
        "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
        "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
        "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device)\n",
        "]\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    result = predict(model, example)\n",
        "    print(f\"Example {idx}\")\n",
        "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
        "    print(f\"Continuation: {result[1:-1]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwi4DhIDzaeU",
        "outputId": "d8285d85-c690-43b9-a32e-b16f27bccf77"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0\n",
            "Input: [0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Continuation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Example 1\n",
            "Input: [1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Example 2\n",
            "Input: [1, 0, 1, 0, 1, 0, 1, 0]\n",
            "Continuation: [1, 0, 1, 0, 1, 0, 1, 0]\n",
            "\n",
            "Example 3\n",
            "Input: [0, 1, 0, 1, 0, 1, 0, 1]\n",
            "Continuation: [1, 0, 1, 0, 1, 0, 1, 0]\n",
            "\n",
            "Example 4\n",
            "Input: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
            "Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
            "\n",
            "Example 5\n",
            "Input: [0, 1]\n",
            "Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}